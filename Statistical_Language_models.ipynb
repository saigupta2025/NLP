{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs5OGfi8gBS1EhJOIqVTa0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saigupta2025/NLP/blob/main/Statistical_Language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import count\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"I love to learn Natural Language Processing using Python.\"\n",
        "print(len(text))\n",
        "\n",
        "# Tokenize text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(len(tokens))\n",
        "\n",
        "# Unigrams (1-gram)\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "print(\"\\nUnigrams:\", unigrams)\n",
        "\n",
        "# Bigrams (2-gram)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"\\nBigrams:\", bigrams)\n",
        "\n",
        "# Trigrams (3-gram)\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"\\nTrigrams:\", trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg_sghxnqr1v",
        "outputId": "dd2dd2f2-dee8-43f4-8a8b-6ccd25826878"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57\n",
            "Tokens: ['I', 'love', 'to', 'learn', 'Natural', 'Language', 'Processing', 'using', 'Python', '.']\n",
            "10\n",
            "\n",
            "Unigrams: [('I',), ('love',), ('to',), ('learn',), ('Natural',), ('Language',), ('Processing',), ('using',), ('Python',), ('.',)]\n",
            "\n",
            "Bigrams: [('I', 'love'), ('love', 'to'), ('to', 'learn'), ('learn', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', 'using'), ('using', 'Python'), ('Python', '.')]\n",
            "\n",
            "Trigrams: [('I', 'love', 'to'), ('love', 'to', 'learn'), ('to', 'learn', 'Natural'), ('learn', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', 'using'), ('Processing', 'using', 'Python'), ('using', 'Python', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram Model From a Text File (Complete Code)"
      ],
      "metadata": {
        "id": "iQlXjXt5vrCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "\n",
        "# STEP 1: READ TEXT FILE\n",
        "\n",
        "file_path = \"/content/input.txt\"   # your file name\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "print(\"File Loaded Successfully!\\n\")\n",
        "\n",
        "# STEP 2: TOKENIZATION\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens, \"\\n\")\n",
        "\n",
        "# STEP 3: UNIGRAM MODEL\n",
        "\n",
        "unigram_freq = FreqDist(tokens)\n",
        "total_unigrams = len(tokens)\n",
        "\n",
        "print(\"===== UNIGRAM PROBABILITIES =====\")\n",
        "for word, count in unigram_freq.items():\n",
        "    prob = count / total_unigrams\n",
        "    print(f\"{word:15s} Count={count:2d}  P(w)={prob:.4f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# STEP 4: BIGRAM MODEL\n",
        "\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "\n",
        "print(\"===== BIGRAM PROBABILITIES =====\")\n",
        "for (w1, w2), count in bigram_freq.items():\n",
        "    prob = count / unigram_freq[w1]   # P(w2 | w1)\n",
        "    print(f\"({w1:10s}, {w2:10s})  Count={count:2d}  P({w2}|{w1})={prob:.4f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# STEP 5: TRIGRAM MODEL\n",
        "\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_freq = FreqDist(trigrams)\n",
        "\n",
        "print(\"===== TRIGRAM PROBABILITIES =====\")\n",
        "for (w1, w2, w3), count in trigram_freq.items():\n",
        "    prob = count / bigram_freq[(w1, w2)]   # P(w3 | w1, w2)\n",
        "    print(f\"({w1}, {w2}, {w3})  Count={count}  P({w3}|{w1},{w2})={prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozcJfPF2vsVw",
        "outputId": "13f781c0-4973-483e-c70a-a5bea287f6e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Loaded Successfully!\n",
            "\n",
            "Tokens: ['natural', 'language', 'processing', 'is', 'good', 'language', 'models', 'help', 'computers', 'understand', 'text', '.', 'nlp', 'is', 'a', 'part', 'of', 'artificial', 'intelligence', '&', 'great', '.'] \n",
            "\n",
            "===== UNIGRAM PROBABILITIES =====\n",
            "natural         Count= 1  P(w)=0.0455\n",
            "language        Count= 2  P(w)=0.0909\n",
            "processing      Count= 1  P(w)=0.0455\n",
            "is              Count= 2  P(w)=0.0909\n",
            "good            Count= 1  P(w)=0.0455\n",
            "models          Count= 1  P(w)=0.0455\n",
            "help            Count= 1  P(w)=0.0455\n",
            "computers       Count= 1  P(w)=0.0455\n",
            "understand      Count= 1  P(w)=0.0455\n",
            "text            Count= 1  P(w)=0.0455\n",
            ".               Count= 2  P(w)=0.0909\n",
            "nlp             Count= 1  P(w)=0.0455\n",
            "a               Count= 1  P(w)=0.0455\n",
            "part            Count= 1  P(w)=0.0455\n",
            "of              Count= 1  P(w)=0.0455\n",
            "artificial      Count= 1  P(w)=0.0455\n",
            "intelligence    Count= 1  P(w)=0.0455\n",
            "&               Count= 1  P(w)=0.0455\n",
            "great           Count= 1  P(w)=0.0455\n",
            "\n",
            "\n",
            "===== BIGRAM PROBABILITIES =====\n",
            "(natural   , language  )  Count= 1  P(language|natural)=1.0000\n",
            "(language  , processing)  Count= 1  P(processing|language)=0.5000\n",
            "(processing, is        )  Count= 1  P(is|processing)=1.0000\n",
            "(is        , good      )  Count= 1  P(good|is)=0.5000\n",
            "(good      , language  )  Count= 1  P(language|good)=1.0000\n",
            "(language  , models    )  Count= 1  P(models|language)=0.5000\n",
            "(models    , help      )  Count= 1  P(help|models)=1.0000\n",
            "(help      , computers )  Count= 1  P(computers|help)=1.0000\n",
            "(computers , understand)  Count= 1  P(understand|computers)=1.0000\n",
            "(understand, text      )  Count= 1  P(text|understand)=1.0000\n",
            "(text      , .         )  Count= 1  P(.|text)=1.0000\n",
            "(.         , nlp       )  Count= 1  P(nlp|.)=0.5000\n",
            "(nlp       , is        )  Count= 1  P(is|nlp)=1.0000\n",
            "(is        , a         )  Count= 1  P(a|is)=0.5000\n",
            "(a         , part      )  Count= 1  P(part|a)=1.0000\n",
            "(part      , of        )  Count= 1  P(of|part)=1.0000\n",
            "(of        , artificial)  Count= 1  P(artificial|of)=1.0000\n",
            "(artificial, intelligence)  Count= 1  P(intelligence|artificial)=1.0000\n",
            "(intelligence, &         )  Count= 1  P(&|intelligence)=1.0000\n",
            "(&         , great     )  Count= 1  P(great|&)=1.0000\n",
            "(great     , .         )  Count= 1  P(.|great)=1.0000\n",
            "\n",
            "\n",
            "===== TRIGRAM PROBABILITIES =====\n",
            "(natural, language, processing)  Count=1  P(processing|natural,language)=1.0000\n",
            "(language, processing, is)  Count=1  P(is|language,processing)=1.0000\n",
            "(processing, is, good)  Count=1  P(good|processing,is)=1.0000\n",
            "(is, good, language)  Count=1  P(language|is,good)=1.0000\n",
            "(good, language, models)  Count=1  P(models|good,language)=1.0000\n",
            "(language, models, help)  Count=1  P(help|language,models)=1.0000\n",
            "(models, help, computers)  Count=1  P(computers|models,help)=1.0000\n",
            "(help, computers, understand)  Count=1  P(understand|help,computers)=1.0000\n",
            "(computers, understand, text)  Count=1  P(text|computers,understand)=1.0000\n",
            "(understand, text, .)  Count=1  P(.|understand,text)=1.0000\n",
            "(text, ., nlp)  Count=1  P(nlp|text,.)=1.0000\n",
            "(., nlp, is)  Count=1  P(is|.,nlp)=1.0000\n",
            "(nlp, is, a)  Count=1  P(a|nlp,is)=1.0000\n",
            "(is, a, part)  Count=1  P(part|is,a)=1.0000\n",
            "(a, part, of)  Count=1  P(of|a,part)=1.0000\n",
            "(part, of, artificial)  Count=1  P(artificial|part,of)=1.0000\n",
            "(of, artificial, intelligence)  Count=1  P(intelligence|of,artificial)=1.0000\n",
            "(artificial, intelligence, &)  Count=1  P(&|artificial,intelligence)=1.0000\n",
            "(intelligence, &, great)  Count=1  P(great|intelligence,&)=1.0000\n",
            "(&, great, .)  Count=1  P(.|&,great)=1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, FreqDist\n",
        "from nltk.util import bigrams\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYX-_MXYp7QF",
        "outputId": "8d988652-2354-4cc4-813e-492021c3a17f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiF7rG20pnD1",
        "outputId": "a661f849-34b4-4f53-90ec-2f9b5921c0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probabilities:\n",
            "NLP 0.2222222222222222\n",
            ". 0.2222222222222222\n",
            "I 0.1111111111111111\n",
            "love 0.1111111111111111\n",
            "learning 0.1111111111111111\n",
            "is 0.1111111111111111\n",
            "interesting 0.1111111111111111\n",
            "\n",
            "Bigrams with Probabilities:\n",
            "('I', 'love') 1.0\n",
            "('love', 'learning') 1.0\n",
            "('learning', 'NLP') 1.0\n",
            "('NLP', '.') 0.5\n",
            "('.', 'NLP') 0.5\n",
            "('NLP', 'is') 0.5\n",
            "('is', 'interesting') 1.0\n",
            "('interesting', '.') 1.0\n"
          ]
        }
      ],
      "source": [
        "text = \"I love learning NLP. NLP is interesting.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Unigram frequencies\n",
        "fdist = FreqDist(tokens)\n",
        "print(\"Unigram Probabilities:\")\n",
        "for word in fdist:\n",
        "    print(word, fdist[word] / len(tokens))\n",
        "\n",
        "# Bigram model\n",
        "print(\"\\nBigrams with Probabilities:\")\n",
        "bigram_list = list(bigrams(tokens))\n",
        "bigram_freq = FreqDist(bigram_list)\n",
        "\n",
        "for bg in bigram_freq:\n",
        "    print(bg, bigram_freq[bg] / fdist[bg[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "\n",
        "text = \"Natural Language Processing is a combo of computer science, Artificial intelligence and language grammar. \"\n",
        "\n",
        "# Step 1: Tokenize\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# UNIGRAM PROBABILITY DISTRIBUTION\n",
        "\n",
        "unigram_freq = FreqDist(tokens)\n",
        "total_unigrams = len(tokens)\n",
        "\n",
        "print(\"----- UNIGRAM PROBABILITIES -----\")\n",
        "for word in unigram_freq:\n",
        "    prob = unigram_freq[word] / total_unigrams\n",
        "    print(f\"P({word}) = {prob:.4f}\")\n",
        "\n",
        "#  BIGRAM PROBABILITY DISTRIBUTION\n",
        "\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "\n",
        "print(\"\\n----- BIGRAM PROBABILITIES -----\")\n",
        "for (w1, w2), freq in bigram_freq.items():\n",
        "    prob = freq / unigram_freq[w1]     # P(w2 | w1)\n",
        "    print(f\"P({w2} | {w1}) = {prob:.4f}\")\n",
        "\n",
        "# TRIGRAM PROBABILITY DISTRIBUTION\n",
        "\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_freq = FreqDist(trigrams)\n",
        "\n",
        "print(\"\\n----- TRIGRAM PROBABILITIES -----\")\n",
        "for (w1, w2, w3), freq in trigram_freq.items():\n",
        "    # denominator = frequency of bigram (w1, w2)\n",
        "    denom = bigram_freq[(w1, w2)]\n",
        "    prob = freq / denom                 # P(w3 | w1, w2)\n",
        "    print(f\"P({w3} | {w1}, {w2}) = {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJG0xdx4twCn",
        "outputId": "d5f0abea-270e-42c8-e23b-872951039ea9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- UNIGRAM PROBABILITIES -----\n",
            "P(language) = 0.1250\n",
            "P(natural) = 0.0625\n",
            "P(processing) = 0.0625\n",
            "P(is) = 0.0625\n",
            "P(a) = 0.0625\n",
            "P(combo) = 0.0625\n",
            "P(of) = 0.0625\n",
            "P(computer) = 0.0625\n",
            "P(science) = 0.0625\n",
            "P(,) = 0.0625\n",
            "P(artificial) = 0.0625\n",
            "P(intelligence) = 0.0625\n",
            "P(and) = 0.0625\n",
            "P(grammar) = 0.0625\n",
            "P(.) = 0.0625\n",
            "\n",
            "----- BIGRAM PROBABILITIES -----\n",
            "P(language | natural) = 1.0000\n",
            "P(processing | language) = 0.5000\n",
            "P(is | processing) = 1.0000\n",
            "P(a | is) = 1.0000\n",
            "P(combo | a) = 1.0000\n",
            "P(of | combo) = 1.0000\n",
            "P(computer | of) = 1.0000\n",
            "P(science | computer) = 1.0000\n",
            "P(, | science) = 1.0000\n",
            "P(artificial | ,) = 1.0000\n",
            "P(intelligence | artificial) = 1.0000\n",
            "P(and | intelligence) = 1.0000\n",
            "P(language | and) = 1.0000\n",
            "P(grammar | language) = 0.5000\n",
            "P(. | grammar) = 1.0000\n",
            "\n",
            "----- TRIGRAM PROBABILITIES -----\n",
            "P(processing | natural, language) = 1.0000\n",
            "P(is | language, processing) = 1.0000\n",
            "P(a | processing, is) = 1.0000\n",
            "P(combo | is, a) = 1.0000\n",
            "P(of | a, combo) = 1.0000\n",
            "P(computer | combo, of) = 1.0000\n",
            "P(science | of, computer) = 1.0000\n",
            "P(, | computer, science) = 1.0000\n",
            "P(artificial | science, ,) = 1.0000\n",
            "P(intelligence | ,, artificial) = 1.0000\n",
            "P(and | artificial, intelligence) = 1.0000\n",
            "P(language | intelligence, and) = 1.0000\n",
            "P(grammar | and, language) = 1.0000\n",
            "P(. | language, grammar) = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "\n",
        "text = \"Natural Language Processing is a combo of computer science, Artificial intelligence and language grammar.\"\n",
        "\n",
        "# Step 1: Tokenize\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# UNIGRAM PROBABILITY TABLE\n",
        "\n",
        "unigram_freq = FreqDist(tokens)\n",
        "total_unigrams = len(tokens)\n",
        "\n",
        "unigram_data = {\n",
        "    'Word': [],\n",
        "    'Count': [],\n",
        "    'Probability P(w)': []\n",
        "}\n",
        "\n",
        "for word, freq in unigram_freq.items():\n",
        "    unigram_data['Word'].append(word)\n",
        "    unigram_data['Count'].append(freq)\n",
        "    unigram_data['Probability P(w)'].append(freq / total_unigrams)\n",
        "\n",
        "df_unigram = pd.DataFrame(unigram_data)\n",
        "print(\"UNIGRAM PROBABILITY TABLE:\\n\")\n",
        "print(df_unigram, \"\\n\")\n",
        "\n",
        "#  BIGRAM PROBABILITY TABLE\n",
        "\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "\n",
        "bigram_data = {\n",
        "    'w1': [],\n",
        "    'w2': [],\n",
        "    'Count (w1,w2)': [],\n",
        "    'Probability P(w2|w1)': []\n",
        "}\n",
        "\n",
        "for (w1, w2), freq in bigram_freq.items():\n",
        "    bigram_data['w1'].append(w1)\n",
        "    bigram_data['w2'].append(w2)\n",
        "    bigram_data['Count (w1,w2)'].append(freq)\n",
        "    bigram_data['Probability P(w2|w1)'].append(freq / unigram_freq[w1])\n",
        "\n",
        "df_bigram = pd.DataFrame(bigram_data)\n",
        "print(\"BIGRAM PROBABILITY TABLE:\\n\")\n",
        "print(df_bigram, \"\\n\")\n",
        "\n",
        "\n",
        "# TRIGRAM PROBABILITY TABLE\n",
        "\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_freq = FreqDist(trigrams)\n",
        "\n",
        "trigram_data = {\n",
        "    'w1': [],\n",
        "    'w2': [],\n",
        "    'w3': [],\n",
        "    'Count (w1,w2,w3)': [],\n",
        "    'Probability P(w3|w1,w2)': []\n",
        "}\n",
        "\n",
        "for (w1, w2, w3), freq in trigram_freq.items():\n",
        "    trigram_data['w1'].append(w1)\n",
        "    trigram_data['w2'].append(w2)\n",
        "    trigram_data['w3'].append(w3)\n",
        "    trigram_data['Count (w1,w2,w3)'].append(freq)\n",
        "    trigram_data['Probability P(w3|w1,w2)'].append(freq / bigram_freq[(w1, w2)])\n",
        "\n",
        "df_trigram = pd.DataFrame(trigram_data)\n",
        "print(\"TRIGRAM PROBABILITY TABLE:\\n\")\n",
        "print(df_trigram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rypo7qRuuojh",
        "outputId": "5f255d63-4e73-4f46-99e1-f286a9d7e7a8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNIGRAM PROBABILITY TABLE:\n",
            "\n",
            "            Word  Count  Probability P(w)\n",
            "0        natural      1            0.0625\n",
            "1       language      2            0.1250\n",
            "2     processing      1            0.0625\n",
            "3             is      1            0.0625\n",
            "4              a      1            0.0625\n",
            "5          combo      1            0.0625\n",
            "6             of      1            0.0625\n",
            "7       computer      1            0.0625\n",
            "8        science      1            0.0625\n",
            "9              ,      1            0.0625\n",
            "10    artificial      1            0.0625\n",
            "11  intelligence      1            0.0625\n",
            "12           and      1            0.0625\n",
            "13       grammar      1            0.0625\n",
            "14             .      1            0.0625 \n",
            "\n",
            "BIGRAM PROBABILITY TABLE:\n",
            "\n",
            "              w1            w2  Count (w1,w2)  Probability P(w2|w1)\n",
            "0        natural      language              1                   1.0\n",
            "1       language    processing              1                   0.5\n",
            "2     processing            is              1                   1.0\n",
            "3             is             a              1                   1.0\n",
            "4              a         combo              1                   1.0\n",
            "5          combo            of              1                   1.0\n",
            "6             of      computer              1                   1.0\n",
            "7       computer       science              1                   1.0\n",
            "8        science             ,              1                   1.0\n",
            "9              ,    artificial              1                   1.0\n",
            "10    artificial  intelligence              1                   1.0\n",
            "11  intelligence           and              1                   1.0\n",
            "12           and      language              1                   1.0\n",
            "13      language       grammar              1                   0.5\n",
            "14       grammar             .              1                   1.0 \n",
            "\n",
            "TRIGRAM PROBABILITY TABLE:\n",
            "\n",
            "              w1            w2            w3  Count (w1,w2,w3)  \\\n",
            "0        natural      language    processing                 1   \n",
            "1       language    processing            is                 1   \n",
            "2     processing            is             a                 1   \n",
            "3             is             a         combo                 1   \n",
            "4              a         combo            of                 1   \n",
            "5          combo            of      computer                 1   \n",
            "6             of      computer       science                 1   \n",
            "7       computer       science             ,                 1   \n",
            "8        science             ,    artificial                 1   \n",
            "9              ,    artificial  intelligence                 1   \n",
            "10    artificial  intelligence           and                 1   \n",
            "11  intelligence           and      language                 1   \n",
            "12           and      language       grammar                 1   \n",
            "13      language       grammar             .                 1   \n",
            "\n",
            "    Probability P(w3|w1,w2)  \n",
            "0                       1.0  \n",
            "1                       1.0  \n",
            "2                       1.0  \n",
            "3                       1.0  \n",
            "4                       1.0  \n",
            "5                       1.0  \n",
            "6                       1.0  \n",
            "7                       1.0  \n",
            "8                       1.0  \n",
            "9                       1.0  \n",
            "10                      1.0  \n",
            "11                      1.0  \n",
            "12                      1.0  \n",
            "13                      1.0  \n"
          ]
        }
      ]
    }
  ]
}